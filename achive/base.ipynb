{"cells":[{"metadata":{},"cell_type":"markdown","source":"# About this kernel"},{"metadata":{},"cell_type":"markdown","source":"in my previous kernel **[Severstal: unet++ with efficientnetb4 keras](https://www.kaggle.com/mobassir/severstal-unet-with-efficientnetb4-keras)** i implemented efficientnetb4 with unet++ using keras.\n\nIn this kernel i will try Nested Unet pytorch from this repository : [Unet-Segmentation-Pytorch-Nest-of-Unets](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets)"},{"metadata":{},"cell_type":"markdown","source":"# Why Nested Unet?"},{"metadata":{},"cell_type":"markdown","source":"Nested unet got highest dice score for hippocampus segmentation ADNI-LONI Dataset(trained by author of this repo : [Unet-Segmentation-Pytorch-Nest-of-Unets](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets)\n![](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets/raw/master/dice.png)\n\n\nAlso Unet Plus Plus introduce intermediate layers to skip connections of U-Net, which naturally form multiple new up-sampling paths from different depths, ensembling U-Nets of various receptive fields. This results in far better performance than traditional Unet. For more details please [refer] .( \"UNet++: A Nested U-Net Architecture for Medical Image Segmentation\")"},{"metadata":{},"cell_type":"markdown","source":"# Plan\n1. using  this kernel :[ UNet starter kernel (Pytorch) LB>0.88](https://www.kaggle.com/rishabhiitbhu/unet-starter-kernel-pytorch-lb-0-88) of @rishabhiitbhu \n\n2. changing and replacing model of @rishabhiitbhu with any of implemented different kinds of Unet Models for Image Segmentation - Unet , RCNN-Unet, Attention Unet, RCNN-Attention Unet, Nested Unet as mentioned above(i will try Nested Unet first)"},{"metadata":{},"cell_type":"markdown","source":"\n<font size=\"4\" color=\"green\">I hope this kernel helpful and some UPVOTES would be very much appreciate!</font>"},{"metadata":{},"cell_type":"markdown","source":"**Note : ** I am completely new in computer vision field,there could be implementation bug in my kernel,please help me in the comment box if you find any,thanks"},{"metadata":{},"cell_type":"markdown","source":"# Types of Unet"},{"metadata":{},"cell_type":"markdown","source":"# Unet\n\n![](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets/blob/master/images/unet1.png?raw=true)\n\n# RCNN Unet\n\n![](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets/blob/master/images/r2unet.png?raw=true)\n\n# Attention Unet\n\n![](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets/blob/master/images/att-unet.png?raw=true)\n\n# Attention-RCNN Unet\n\n![](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets/blob/master/images/att-r2u.png?raw=true)\n\n# Nested Unet\n\n![](https://github.com/bigmb/Unet-Segmentation-Pytorch-Nest-of-Unets/blob/master/images/nested.jpg?raw=true)\n"},{"metadata":{},"cell_type":"markdown","source":"**Version 5 :  i will train for only 1 epoch as i don't have much gpu quota left,i will also use batch_size = 2 and pass 1 train and 1 valid image as more than that can't be passed using kaggle GPU,if you have got good GPU you can try BIG**"},{"metadata":{},"cell_type":"markdown","source":"## Imports"},{"metadata":{"trusted":true},"cell_type":"code","source":"import os\nimport cv2\nimport pdb\nimport time\nimport warnings\nimport random\nimport numpy as np\nimport pandas as pd\nfrom tqdm import tqdm_notebook as tqdm\nfrom torch.optim.lr_scheduler import ReduceLROnPlateau\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport torch.nn as nn\nfrom torch.nn import functional as F\nimport torch.optim as optim\nimport torch.backends.cudnn as cudnn\nfrom torch.utils.data import DataLoader, Dataset, sampler\nfrom matplotlib import pyplot as plt\nfrom albumentations import (HorizontalFlip, ShiftScaleRotate, Normalize, Resize, Compose, GaussNoise)\nfrom albumentations.pytorch import ToTensor\nwarnings.filterwarnings(\"ignore\")\nseed = 43\nrandom.seed(seed)\nos.environ[\"PYTHONHASHSEED\"] = str(seed)\nnp.random.seed(seed)\ntorch.cuda.manual_seed(seed)\ntorch.backends.cudnn.deterministic = True\n\n!pip install torchsummary\nimport torchsummary","execution_count":98,"outputs":[{"output_type":"stream","text":"Requirement already satisfied: torchsummary in /opt/conda/lib/python3.6/site-packages (1.5.1)\r\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"!cp -r ../input/pytorch-unet-segmentation/Unet-Segmentation-Pytorch-Nest-of-Unets-master/* ./\n","execution_count":99,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from Models import Unet_dict, NestedUNet, U_Net, R2U_Net, AttU_Net, R2AttU_Net","execution_count":100,"outputs":[]},{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"!pip install ../input/pretrainedmodels/pretrainedmodels-0.7.4/pretrainedmodels-0.7.4/ > /dev/null # no output\npackage_path = '../input/unetmodelscript' # add unet script dataset\nimport sys\nsys.path.append(package_path)\n#from model import Unet # import Unet model from the script\n#!pip install git+https://github.com/qubvel/segmentation_models.pytorch","execution_count":101,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## RLE-Mask utility functions"},{"metadata":{"trusted":true},"cell_type":"code","source":"#https://www.kaggle.com/paulorzp/rle-functions-run-lenght-encode-decode\ndef mask2rle(img):\n    '''\n    img: numpy array, 1 -> mask, 0 -> background\n    Returns run length as string formated\n    '''\n    pixels= img.T.flatten()\n    pixels = np.concatenate([[0], pixels, [0]])\n    runs = np.where(pixels[1:] != pixels[:-1])[0] + 1\n    runs[1::2] -= runs[::2]\n    return ' '.join(str(x) for x in runs)\n\ndef make_mask(row_id, df):\n    '''Given a row index, return image_id and mask (256, 1600, 4) from the dataframe `df`'''\n    fname = df.iloc[row_id].name\n    labels = df.iloc[row_id][:4]\n    masks = np.zeros((256, 1600, 4), dtype=np.float32) # float32 is V.Imp\n    # 4:class 1～4 (ch:0～3)\n\n    for idx, label in enumerate(labels.values):\n        if label is not np.nan:\n            label = label.split(\" \")\n            positions = map(int, label[0::2])\n            length = map(int, label[1::2])\n            mask = np.zeros(256 * 1600, dtype=np.uint8)\n            for pos, le in zip(positions, length):\n                mask[pos:(pos + le)] = 1\n            masks[:, :, idx] = mask.reshape(256, 1600, order='F')\n    return fname, masks","execution_count":102,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Dataloader"},{"metadata":{"trusted":true},"cell_type":"code","source":"class SteelDataset(Dataset):\n    def __init__(self, df, data_folder, mean, std, phase):\n        self.df = df\n        self.root = data_folder\n        self.mean = mean\n        self.std = std\n        self.phase = phase\n        self.transforms = get_transforms(phase, mean, std)\n        self.fnames = self.df.index.tolist()\n\n    def __getitem__(self, idx):\n        image_id, mask = make_mask(idx, self.df)\n        image_path = os.path.join(self.root, \"train_images\",  image_id)\n        img = cv2.imread(image_path)\n        augmented = self.transforms(image=img, mask=mask)\n        img = augmented['image']\n        mask = augmented['mask'] # 1x256x1600x4\n        mask = mask[0].permute(2, 0, 1) # 1x4x256x1600\n        return img, mask\n\n    def __len__(self):\n        return len(self.fnames)\n\n\ndef get_transforms(phase, mean, std):\n    list_transforms = []\n    if phase == \"train\":\n        list_transforms.extend(\n            [\n                HorizontalFlip(p=0.5), # only horizontal flip as of now\n            ]\n        )\n    list_transforms.extend(\n        [\n            Normalize(mean=mean, std=std, p=1),\n            ToTensor(),\n        ]\n    )\n    list_trfms = Compose(list_transforms)\n    return list_trfms\n\ndef provider(\n    data_folder,\n    df_path,\n    phase,\n    mean=None,\n    std=None,\n    batch_size=2,\n    num_workers=4,\n):\n    '''Returns dataloader for the model training'''\n    df = pd.read_csv(df_path)\n    # https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n    df['ImageId'], df['ClassId'] = zip(*df['ImageId_ClassId'].str.split('_'))\n    df['ClassId'] = df['ClassId'].astype(int)\n    df = df.pivot(index='ImageId',columns='ClassId',values='EncodedPixels')\n    df['defects'] = df.count(axis=1)\n    \n    train_df, val_df = train_test_split(df, test_size=0.15, stratify=df[\"defects\"], random_state=69)\n    df = train_df if phase == \"train\" else val_df\n    image_dataset = SteelDataset(df, data_folder, mean, std, phase)\n    dataloader = DataLoader(\n        image_dataset,\n        batch_size=batch_size,\n        num_workers=num_workers,\n        pin_memory=True,\n        shuffle=True,   \n    )\n\n    return dataloader\n\n","execution_count":103,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Some more utility functions\n\nDice and IoU metric implementations, metric logger for training and validation."},{"metadata":{"trusted":true},"cell_type":"code","source":"def predict(X, threshold):\n    '''X is sigmoid output of the model'''\n    X_p = np.copy(X)\n    preds = (X_p > threshold).astype('uint8')\n    return preds\n\ndef metric(probability, truth, threshold=0.5, reduction='none'):\n    '''Calculates dice of positive and negative images seperately'''\n    '''probability and truth must be torch tensors'''\n    batch_size = len(truth)\n    with torch.no_grad():\n        probability = probability.view(batch_size, -1)\n        truth = truth.view(batch_size, -1)\n        assert(probability.shape == truth.shape)\n\n        p = (probability > threshold).float()\n        t = (truth > 0.5).float()\n\n        t_sum = t.sum(-1)\n        p_sum = p.sum(-1)\n        neg_index = torch.nonzero(t_sum == 0)\n        pos_index = torch.nonzero(t_sum >= 1)\n\n        dice_neg = (p_sum == 0).float()\n        dice_pos = 2 * (p*t).sum(-1)/((p+t).sum(-1))\n\n        dice_neg = dice_neg[neg_index]\n        dice_pos = dice_pos[pos_index]\n        dice = torch.cat([dice_pos, dice_neg])\n\n        dice_neg = np.nan_to_num(dice_neg.mean().item(), 0)\n        dice_pos = np.nan_to_num(dice_pos.mean().item(), 0)\n        dice = dice.mean().item()\n\n        num_neg = len(neg_index)\n        num_pos = len(pos_index)\n\n    return dice, dice_neg, dice_pos, num_neg, num_pos\n\nclass Meter:\n    '''A meter to keep track of iou and dice scores throughout an epoch'''\n    def __init__(self, phase, epoch):\n        self.base_threshold = 0.5 # <<<<<<<<<<< here's the threshold\n        self.base_dice_scores = []\n        self.dice_neg_scores = []\n        self.dice_pos_scores = []\n        self.iou_scores = []\n\n    def update(self, targets, outputs):\n        probs = torch.sigmoid(outputs)\n        dice, dice_neg, dice_pos, _, _ = metric(probs, targets, self.base_threshold)\n        self.base_dice_scores.append(dice)\n        self.dice_pos_scores.append(dice_pos)\n        self.dice_neg_scores.append(dice_neg)\n        preds = predict(probs, self.base_threshold)\n        iou = compute_iou_batch(preds, targets, classes=[1])\n        self.iou_scores.append(iou)\n\n    def get_metrics(self):\n        dice = np.mean(self.base_dice_scores)\n        dice_neg = np.mean(self.dice_neg_scores)\n        dice_pos = np.mean(self.dice_pos_scores)\n        dices = [dice, dice_neg, dice_pos]\n        iou = np.nanmean(self.iou_scores)\n        return dices, iou\n\ndef epoch_log(phase, epoch, epoch_loss, meter, start):\n    '''logging the metrics at the end of an epoch'''\n    dices, iou = meter.get_metrics()\n    dice, dice_neg, dice_pos = dices\n    print(\"Loss: %0.4f | IoU: %0.4f | dice: %0.4f | dice_neg: %0.4f | dice_pos: %0.4f\" % (epoch_loss, iou, dice, dice_neg, dice_pos))\n    return dice, iou\n\ndef compute_ious(pred, label, classes, ignore_index=255, only_present=True):\n    '''computes iou for one ground truth mask and predicted mask'''\n    pred[label == ignore_index] = 0\n    ious = []\n    for c in classes:\n        label_c = label == c\n        if only_present and np.sum(label_c) == 0:\n            ious.append(np.nan)\n            continue\n        pred_c = pred == c\n        intersection = np.logical_and(pred_c, label_c).sum()\n        union = np.logical_or(pred_c, label_c).sum()\n        if union != 0:\n            ious.append(intersection / union)\n    return ious if ious else [1]\n\ndef compute_iou_batch(outputs, labels, classes=None):\n    '''computes mean iou for a batch of ground truth masks and predicted masks'''\n    ious = []\n    preds = np.copy(outputs) # copy is imp\n    labels = np.array(labels) # tensor to np\n    for pred, label in zip(preds, labels):\n        ious.append(np.nanmean(compute_ious(pred, label, classes)))\n    iou = np.nanmean(ious)\n    return iou\n","execution_count":104,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Model Initialization"},{"metadata":{"trusted":true},"cell_type":"code","source":"#!mkdir -p /tmp/.cache/torch/checkpoints/\n#!cp ../input/resnet18/resnet18.pth /tmp/.cache/torch/checkpoints/resnet18-5c106cde.pth","execution_count":105,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_on_gpu = torch.cuda.is_available()\n\nif not train_on_gpu:\n    print('CUDA is not available. Training on CPU')\nelse:\n    print('CUDA is available. Training on GPU')\n\ndevice = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")","execution_count":106,"outputs":[{"output_type":"stream","text":"CUDA is available. Training on GPU\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"\nmodel_Inputs = [U_Net, R2U_Net, AttU_Net, R2AttU_Net, NestedUNet]","execution_count":107,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def model_unet(model_input, in_channel=3, out_channel=1):\n    model_test = model_input(in_channel, out_channel)\n    return model_test\n\nmodel_test = model_unet(model_Inputs[4], 3, 4)\n\nmodel_test.to(device)\ntorchsummary.summary(model_test, input_size=(3, 256, 1600))","execution_count":108,"outputs":[{"output_type":"stream","text":"----------------------------------------------------------------\n        Layer (type)               Output Shape         Param #\n================================================================\n            Conv2d-1        [-1, 64, 256, 1600]           1,792\n       BatchNorm2d-2        [-1, 64, 256, 1600]             128\n              ReLU-3        [-1, 64, 256, 1600]               0\n            Conv2d-4        [-1, 64, 256, 1600]          36,928\n       BatchNorm2d-5        [-1, 64, 256, 1600]             128\n              ReLU-6        [-1, 64, 256, 1600]               0\n conv_block_nested-7        [-1, 64, 256, 1600]               0\n         MaxPool2d-8         [-1, 64, 128, 800]               0\n            Conv2d-9        [-1, 128, 128, 800]          73,856\n      BatchNorm2d-10        [-1, 128, 128, 800]             256\n             ReLU-11        [-1, 128, 128, 800]               0\n           Conv2d-12        [-1, 128, 128, 800]         147,584\n      BatchNorm2d-13        [-1, 128, 128, 800]             256\n             ReLU-14        [-1, 128, 128, 800]               0\nconv_block_nested-15        [-1, 128, 128, 800]               0\n         Upsample-16       [-1, 128, 256, 1600]               0\n           Conv2d-17        [-1, 64, 256, 1600]         110,656\n      BatchNorm2d-18        [-1, 64, 256, 1600]             128\n             ReLU-19        [-1, 64, 256, 1600]               0\n           Conv2d-20        [-1, 64, 256, 1600]          36,928\n      BatchNorm2d-21        [-1, 64, 256, 1600]             128\n             ReLU-22        [-1, 64, 256, 1600]               0\nconv_block_nested-23        [-1, 64, 256, 1600]               0\n        MaxPool2d-24         [-1, 128, 64, 400]               0\n           Conv2d-25         [-1, 256, 64, 400]         295,168\n      BatchNorm2d-26         [-1, 256, 64, 400]             512\n             ReLU-27         [-1, 256, 64, 400]               0\n           Conv2d-28         [-1, 256, 64, 400]         590,080\n      BatchNorm2d-29         [-1, 256, 64, 400]             512\n             ReLU-30         [-1, 256, 64, 400]               0\nconv_block_nested-31         [-1, 256, 64, 400]               0\n         Upsample-32        [-1, 256, 128, 800]               0\n           Conv2d-33        [-1, 128, 128, 800]         442,496\n      BatchNorm2d-34        [-1, 128, 128, 800]             256\n             ReLU-35        [-1, 128, 128, 800]               0\n           Conv2d-36        [-1, 128, 128, 800]         147,584\n      BatchNorm2d-37        [-1, 128, 128, 800]             256\n             ReLU-38        [-1, 128, 128, 800]               0\nconv_block_nested-39        [-1, 128, 128, 800]               0\n         Upsample-40       [-1, 128, 256, 1600]               0\n           Conv2d-41        [-1, 64, 256, 1600]         147,520\n      BatchNorm2d-42        [-1, 64, 256, 1600]             128\n             ReLU-43        [-1, 64, 256, 1600]               0\n           Conv2d-44        [-1, 64, 256, 1600]          36,928\n      BatchNorm2d-45        [-1, 64, 256, 1600]             128\n             ReLU-46        [-1, 64, 256, 1600]               0\nconv_block_nested-47        [-1, 64, 256, 1600]               0\n        MaxPool2d-48         [-1, 256, 32, 200]               0\n           Conv2d-49         [-1, 512, 32, 200]       1,180,160\n      BatchNorm2d-50         [-1, 512, 32, 200]           1,024\n             ReLU-51         [-1, 512, 32, 200]               0\n           Conv2d-52         [-1, 512, 32, 200]       2,359,808\n      BatchNorm2d-53         [-1, 512, 32, 200]           1,024\n             ReLU-54         [-1, 512, 32, 200]               0\nconv_block_nested-55         [-1, 512, 32, 200]               0\n         Upsample-56         [-1, 512, 64, 400]               0\n           Conv2d-57         [-1, 256, 64, 400]       1,769,728\n      BatchNorm2d-58         [-1, 256, 64, 400]             512\n             ReLU-59         [-1, 256, 64, 400]               0\n           Conv2d-60         [-1, 256, 64, 400]         590,080\n      BatchNorm2d-61         [-1, 256, 64, 400]             512\n             ReLU-62         [-1, 256, 64, 400]               0\nconv_block_nested-63         [-1, 256, 64, 400]               0\n         Upsample-64        [-1, 256, 128, 800]               0\n           Conv2d-65        [-1, 128, 128, 800]         589,952\n      BatchNorm2d-66        [-1, 128, 128, 800]             256\n             ReLU-67        [-1, 128, 128, 800]               0\n           Conv2d-68        [-1, 128, 128, 800]         147,584\n      BatchNorm2d-69        [-1, 128, 128, 800]             256\n             ReLU-70        [-1, 128, 128, 800]               0\nconv_block_nested-71        [-1, 128, 128, 800]               0\n         Upsample-72       [-1, 128, 256, 1600]               0\n           Conv2d-73        [-1, 64, 256, 1600]         184,384\n      BatchNorm2d-74        [-1, 64, 256, 1600]             128\n             ReLU-75        [-1, 64, 256, 1600]               0\n           Conv2d-76        [-1, 64, 256, 1600]          36,928\n      BatchNorm2d-77        [-1, 64, 256, 1600]             128\n             ReLU-78        [-1, 64, 256, 1600]               0\nconv_block_nested-79        [-1, 64, 256, 1600]               0\n        MaxPool2d-80         [-1, 512, 16, 100]               0\n           Conv2d-81        [-1, 1024, 16, 100]       4,719,616\n      BatchNorm2d-82        [-1, 1024, 16, 100]           2,048\n             ReLU-83        [-1, 1024, 16, 100]               0\n           Conv2d-84        [-1, 1024, 16, 100]       9,438,208\n      BatchNorm2d-85        [-1, 1024, 16, 100]           2,048\n             ReLU-86        [-1, 1024, 16, 100]               0\nconv_block_nested-87        [-1, 1024, 16, 100]               0\n         Upsample-88        [-1, 1024, 32, 200]               0\n           Conv2d-89         [-1, 512, 32, 200]       7,078,400\n      BatchNorm2d-90         [-1, 512, 32, 200]           1,024\n             ReLU-91         [-1, 512, 32, 200]               0\n           Conv2d-92         [-1, 512, 32, 200]       2,359,808\n      BatchNorm2d-93         [-1, 512, 32, 200]           1,024\n             ReLU-94         [-1, 512, 32, 200]               0\nconv_block_nested-95         [-1, 512, 32, 200]               0\n         Upsample-96         [-1, 512, 64, 400]               0\n           Conv2d-97         [-1, 256, 64, 400]       2,359,552\n      BatchNorm2d-98         [-1, 256, 64, 400]             512\n             ReLU-99         [-1, 256, 64, 400]               0\n          Conv2d-100         [-1, 256, 64, 400]         590,080\n     BatchNorm2d-101         [-1, 256, 64, 400]             512\n            ReLU-102         [-1, 256, 64, 400]               0\nconv_block_nested-103         [-1, 256, 64, 400]               0\n        Upsample-104        [-1, 256, 128, 800]               0\n          Conv2d-105        [-1, 128, 128, 800]         737,408\n     BatchNorm2d-106        [-1, 128, 128, 800]             256\n            ReLU-107        [-1, 128, 128, 800]               0\n          Conv2d-108        [-1, 128, 128, 800]         147,584\n     BatchNorm2d-109        [-1, 128, 128, 800]             256\n            ReLU-110        [-1, 128, 128, 800]               0\nconv_block_nested-111        [-1, 128, 128, 800]               0\n        Upsample-112       [-1, 128, 256, 1600]               0\n          Conv2d-113        [-1, 64, 256, 1600]         221,248\n     BatchNorm2d-114        [-1, 64, 256, 1600]             128\n            ReLU-115        [-1, 64, 256, 1600]               0\n          Conv2d-116        [-1, 64, 256, 1600]          36,928\n     BatchNorm2d-117        [-1, 64, 256, 1600]             128\n            ReLU-118        [-1, 64, 256, 1600]               0\nconv_block_nested-119        [-1, 64, 256, 1600]               0\n          Conv2d-120         [-1, 4, 256, 1600]             260\n================================================================\nTotal params: 36,629,828\nTrainable params: 36,629,828\nNon-trainable params: 0\n----------------------------------------------------------------\nInput size (MB): 4.69\nForward/backward pass size (MB): 13843.75\nParams size (MB): 139.73\nEstimated Total Size (MB): 13988.17\n----------------------------------------------------------------\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"model = model_test","execution_count":109,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#model # a *deeper* look","execution_count":110,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"### Training and Validation"},{"metadata":{"trusted":true},"cell_type":"code","source":"class Trainer(object):\n    '''This class takes care of training and validation of our model'''\n    def __init__(self, model):\n        self.num_workers = 4\n        self.batch_size = {\"train\": 1, \"val\": 1}\n        self.accumulation_steps = 32 // self.batch_size['train']\n        self.lr = 0.00001\n        self.num_epochs = 1\n        self.best_loss = float(\"inf\")\n        self.phases = [\"train\", \"val\"]\n        self.device = torch.device(\"cuda:0\")\n        torch.set_default_tensor_type(\"torch.cuda.FloatTensor\")\n        self.net = model\n        self.criterion = torch.nn.BCEWithLogitsLoss()\n        self.optimizer = optim.Adam(self.net.parameters(), lr=self.lr)\n        self.scheduler = ReduceLROnPlateau(self.optimizer, mode=\"min\", patience=5, verbose=True)\n        self.net = self.net.to(self.device)\n        cudnn.benchmark = True\n        self.dataloaders = {\n            phase: provider(\n                data_folder=data_folder,\n                df_path=train_df_path,\n                phase=phase,\n                mean=(0.485, 0.456, 0.406),\n                std=(0.229, 0.224, 0.225),\n                batch_size=self.batch_size[phase],\n                num_workers=self.num_workers,\n            )\n            for phase in self.phases\n        }\n        self.losses = {phase: [] for phase in self.phases}\n        self.iou_scores = {phase: [] for phase in self.phases}\n        self.dice_scores = {phase: [] for phase in self.phases}\n        \n    def forward(self, images, targets):\n        images = images.to(self.device)\n        masks = targets.to(self.device)\n        outputs = self.net(images)\n        loss = self.criterion(outputs, masks)\n        return loss, outputs\n\n    def iterate(self, epoch, phase):\n        meter = Meter(phase, epoch)\n        start = time.strftime(\"%H:%M:%S\")\n        print(f\"Starting epoch: {epoch} | phase: {phase} | ⏰: {start}\")\n        batch_size = self.batch_size[phase]\n        self.net.train(phase == \"train\")\n        dataloader = self.dataloaders[phase]\n        running_loss = 0.0\n        total_batches = len(dataloader)\n#         tk0 = tqdm(dataloader, total=total_batches)\n        self.optimizer.zero_grad()\n        for itr, batch in enumerate(dataloader): # replace `dataloader` with `tk0` for tqdm\n            images, targets = batch\n            loss, outputs = self.forward(images, targets)\n            loss = loss / self.accumulation_steps\n            if phase == \"train\":\n                loss.backward()\n                if (itr + 1 ) % self.accumulation_steps == 0:\n                    self.optimizer.step()\n                    self.optimizer.zero_grad()\n            running_loss += loss.item()\n            outputs = outputs.detach().cpu()\n            meter.update(targets, outputs)\n            #tk0.set_postfix(loss=(running_loss / ((itr + 1))))\n        epoch_loss = (running_loss * self.accumulation_steps) / total_batches\n        dice, iou = epoch_log(phase, epoch, epoch_loss, meter, start)\n        self.losses[phase].append(epoch_loss)\n        self.dice_scores[phase].append(dice)\n        self.iou_scores[phase].append(iou)\n        torch.cuda.empty_cache()\n        return epoch_loss\n\n    def start(self):\n        for epoch in range(self.num_epochs):\n            self.iterate(epoch, \"train\")\n            state = {\n                \"epoch\": epoch,\n                \"best_loss\": self.best_loss,\n                \"state_dict\": self.net.state_dict(),\n                \"optimizer\": self.optimizer.state_dict(),\n            }\n            with torch.no_grad():\n                val_loss = self.iterate(epoch, \"val\")\n                self.scheduler.step(val_loss)\n            if val_loss < self.best_loss:\n                print(\"******** New optimal found, saving state ********\")\n                state[\"best_loss\"] = self.best_loss = val_loss\n                torch.save(state, \"./model.pth\")\n            print()\n","execution_count":111,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"sample_submission_path = '../input/severstal-steel-defect-detection/sample_submission.csv'\ntrain_df_path = '../input/severstal-steel-defect-detection/train.csv'\ndata_folder = \"../input/severstal-steel-defect-detection/\"\ntest_data_folder = \"../input/severstal-steel-defect-detection/test_images\"","execution_count":112,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"model_trainer = Trainer(model)\nmodel_trainer.start()","execution_count":null,"outputs":[{"output_type":"stream","text":"Starting epoch: 0 | phase: train | ⏰: 02:30:29\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"# PLOT TRAINING\nlosses = model_trainer.losses\ndice_scores = model_trainer.dice_scores # overall dice\niou_scores = model_trainer.iou_scores\n\ndef plot(scores, name):\n    plt.figure(figsize=(15,5))\n    plt.plot(range(len(scores[\"train\"])), scores[\"train\"], label=f'train {name}')\n    plt.plot(range(len(scores[\"train\"])), scores[\"val\"], label=f'val {name}')\n    plt.title(f'{name} plot'); plt.xlabel('Epoch'); plt.ylabel(f'{name}');\n    plt.legend(); \n    plt.show()\n\nplot(losses, \"BCE loss\")\nplot(dice_scores, \"Dice score\")\nplot(iou_scores, \"IoU score\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Test prediction and submission"},{"metadata":{},"cell_type":"markdown","source":"I'll try inference kernel when time permits"},{"metadata":{},"cell_type":"markdown","source":"## Refrences:\n\nFew kernels from which I've borrowed some cod[](http://)e:\n\n* https://www.kaggle.com/amanooo/defect-detection-starter-u-net\n* https://www.kaggle.com/go1dfish/clear-mask-visualization-and-simple-eda\n\nA big thank you to all those who share their code on Kaggle, I'm nobody without you guys. I've learnt a lot from fellow kagglers, special shout-out to [@Abhishek](https://www.kaggle.com/abhishek), [@Yury](https://www.kaggle.com/deyury), [@Heng](https://www.kaggle.com/hengck23), [@Ekhtiar](https://www.kaggle.com/ekhtiar), [@lafoss](https://www.kaggle.com/iafoss), [@Siddhartha](https://www.kaggle.com/meaninglesslives), [@xhulu](https://www.kaggle.com/xhlulu), and the list goes on.."}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":1}